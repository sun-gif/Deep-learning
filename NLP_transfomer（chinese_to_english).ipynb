{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-transfomer（chinese to english).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbH5+oQ34/aOfdkmecJQCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sun-gif/Deep-learning/blob/master/NLP_transfomer%EF%BC%88chinese_to_english).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghk_rezDFoKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920e8436-cbac-45be-ffae-0b4eca4d63ab"
      },
      "source": [
        "!pip uninstall spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling spacy-2.2.4:\n",
            "  Would remove:\n",
            "    /usr/local/bin/spacy\n",
            "    /usr/local/lib/python3.7/dist-packages/bin/*\n",
            "    /usr/local/lib/python3.7/dist-packages/spacy-2.2.4.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/spacy/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/bin/theano_cache.py\n",
            "    /usr/local/lib/python3.7/dist-packages/bin/theano_nose.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled spacy-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-bjhS_6F7hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6d82b2-9e86-483c-bd5e-3fef3beb90c2"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 229kB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.8.1)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/54/76982427ceb495dd19ff982c966708c624b85e03c45bf1912feaf60c7b2d/srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 50.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 60.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=5f23f4d247d9cf1d2daf7d26e923c3d6131c6d14060ac55ff3ef87c1e04930d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: typer, smart-open, pathy, spacy-legacy, catalogue, srsly, pydantic, thinc, spacy\n",
            "  Found existing installation: smart-open 4.2.0\n",
            "    Uninstalling smart-open-4.2.0:\n",
            "      Successfully uninstalled smart-open-4.2.0\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.2 srsly-2.4.0 thinc-8.0.2 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg_0AeTmuWPM"
      },
      "source": [
        "\n",
        "import torch\n",
        "import spacy\n",
        "from torchtext.data.metrics import bleu_score\n",
        "import sys\n",
        "\n",
        "\n",
        "def translate_sentence(model, sentence, chinese, english, device, max_length=50):\n",
        "    # Load german tokenizer\n",
        "    spacy_che = spacy.load(\"zh_core_web_sm\")\n",
        "\n",
        "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_che(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\n",
        "    tokens.insert(0, chinese.init_token)\n",
        "    tokens.append(chinese.eos_token)\n",
        "\n",
        "    # Go through each german token and convert to an index\n",
        "    text_to_indices = [chinese.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "    for i in range(max_length):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(sentence_tensor, trg_tensor)\n",
        "\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    # remove start token\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "\n",
        "def bleu(data, model, chinese, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"che\"]\n",
        "        trg = vars(example)[\"eng\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, chinese, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdT6oDkeulO6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "\n",
        "import torchtext\n",
        "#from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator,TabularDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "XAAKIIctpE4K",
        "outputId": "6d5bd040-ddc2-4074-f90d-9a422705e348"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0bd2f86d-a7bb-4821-8f2d-040cde76bb0e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0bd2f86d-a7bb-4821-8f2d-040cde76bb0e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving cmn.txt to cmn.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohQymdAmwJPL"
      },
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mon9h9_s0QbJ",
        "outputId": "e6d08423-09b9-4ad5-f3d4-78d386c25090"
      },
      "source": [
        "import pandas as pd\n",
        "file_name='cmn.txt'\n",
        "lines = uploaded[file_name].decode(\"utf-8\").split(\"\\n\")\n",
        "pairs=[line.split('\\t')[:2] for line in lines]\n",
        "df=pd.DataFrame(pairs[:20000],columns=['english','chinese'])\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ9ndiElx7G1"
      },
      "source": [
        "# create train and test set\n",
        "train, test = train_test_split(df, test_size=0.2)\n",
        "\n",
        "# Get train, test data to json and csv format which can be read by torchtext\n",
        "train.to_json(\"train.json\", orient=\"records\", lines=True)\n",
        "test.to_json(\"test.json\", orient=\"records\", lines=True)\n",
        "\n",
        "train.to_csv(\"train.csv\", index=False)\n",
        "test.to_csv(\"test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZKpz6mFvohD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013fa95a-86b3-416b-f471-dba31e811d1c"
      },
      "source": [
        "\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download zh_core_web_sm\n",
        "\n",
        "spacy_che = spacy.load(\"zh_core_web_sm\")\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "def tokenize_che(text):\n",
        "    return [tok.text for tok in spacy_che.tokenizer(text)]\n",
        "\n",
        "\n",
        "def tokenize_eng(text):\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "\n",
        "chinese = Field(tokenize=tokenize_che, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "english = Field(\n",
        "    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
        ")\n",
        "\n",
        "#train_data, valid_data, test_data = pairs.splits(\n",
        "#    exts=(\".ch\", \".en\"), fields=(chinese, english)\n",
        "#)\n",
        "\n",
        "#chinese.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "#english.build_vocab(train_data, max_size=10000, min_freq=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-11 17:11:25.279543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 243kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.8.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2021-04-11 17:11:33.011927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting zh-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.0.0/zh_core_web_sm-3.0.0-py3-none-any.whl (49.5MB)\n",
            "\u001b[K     |████████████████████████████████| 49.6MB 57kB/s \n",
            "\u001b[?25hCollecting spacy-pkuseg<0.1.0,>=0.0.27\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/c1/c8c5e08eb4c3108c3a0a412955356fdecbb8ceddc49611a54d444c91b014/spacy_pkuseg-0.0.28-cp37-cp37m-manylinux2014_x86_64.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 15.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy-pkuseg<0.1.0,>=0.0.27->zh-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from spacy-pkuseg<0.1.0,>=0.0.27->zh-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from spacy-pkuseg<0.1.0,>=0.0.27->zh-core-web-sm==3.0.0) (0.29.22)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.8.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2020.12.5)\n",
            "Installing collected packages: spacy-pkuseg, zh-core-web-sm\n",
            "Successfully installed spacy-pkuseg-0.0.28 zh-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8MG4p9BmqxO"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size=32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3wgsgxiydkj",
        "outputId": "53c4dc50-49f2-4a8c-9aff-62bcbaa102fa"
      },
      "source": [
        "fields = {'english': (\"eng\", english),'chinese': (\"che\", chinese)}\n",
        "\n",
        "train_data, test_data = TabularDataset.splits(\n",
        "    path=\"\", train=\"train.json\", test=\"test.json\", format=\"json\", fields=fields\n",
        ")\n",
        "\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "chinese.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "train_iterator,  test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data),\n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.che),\n",
        "   \n",
        "    device=device,\n",
        ")\n",
        "\n",
        "\n",
        "for batch in train_iterator:\n",
        "    print(batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 15x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 16x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 18x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 4x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 16x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 16x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 15x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 17x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 16x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 15x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 17x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 15x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 16x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 19x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 17x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 16x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 14x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 15x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 16x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 5x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 7x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 13x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 11x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 8x32 (GPU 0)]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.eng]:[torch.cuda.LongTensor of size 12x32 (GPU 0)]\n",
            "\t[.che]:[torch.cuda.LongTensor of size 9x32 (GPU 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WhPFl97wPyq"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_len,\n",
        "        device,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "\n",
        "        self.device = device\n",
        "        self.transformer = nn.Transformer(\n",
        "            embedding_size,\n",
        "            num_heads,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "\n",
        "        # (N, src_len)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_seq_length, N = src.shape\n",
        "        trg_seq_length, N = trg.shape\n",
        "\n",
        "        src_positions = (\n",
        "            torch.arange(0, src_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(src_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        trg_positions = (\n",
        "            torch.arange(0, trg_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(trg_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        embed_src = self.dropout(\n",
        "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
        "        )\n",
        "        embed_trg = self.dropout(\n",
        "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
        "        )\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        out = self.transformer(\n",
        "            embed_src,\n",
        "            embed_trg,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_mask=trg_mask,\n",
        "        )\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7v4dxOUKBaq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc3zzccEwiJi"
      },
      "source": [
        "\n",
        "# We're ready to define everything we need for training our Seq2Seq model\n",
        "\n",
        "\n",
        "load_model = True\n",
        "save_model = True\n",
        "\n",
        "# Training hyperparameters\n",
        "num_epochs = 1000\n",
        "learning_rate = 3e-4\n",
        "batch_size = 32\n",
        "\n",
        "# Model hyperparameters\n",
        "src_vocab_size = len(chinese.vocab)\n",
        "trg_vocab_size = len(english.vocab)\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.10\n",
        "max_len = 100\n",
        "forward_expansion = 4\n",
        "src_pad_idx = chinese.vocab.stoi[\"<pad>\"]\n",
        "\n",
        "# Tensorboard to get nice loss plot\n",
        "writer = SummaryWriter(\"runs/loss_plot\")\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go-3ilaUwoPN"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "model = Transformer(\n",
        "    embedding_size,\n",
        "    src_vocab_size,\n",
        "    trg_vocab_size,\n",
        "    src_pad_idx,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    forward_expansion,\n",
        "    dropout,\n",
        "    max_len,\n",
        "    device,\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, factor=0.1, patience=10, verbose=True\n",
        ")\n",
        "\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmbRlK2wub4h",
        "outputId": "aa7c5fc7-6638-4432-8a47-5bd36476182e"
      },
      "source": [
        "\n",
        "\n",
        "sentence = \"明天会更好\"\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        # Get input and targets and get to cuda\n",
        "        inp_data = batch.che.to(device)\n",
        "        target = batch.eng.to(device)\n",
        "       \n",
        "\n",
        "        # Forward prop\n",
        "        output = model(inp_data, target[:-1])\n",
        "\n",
        "       \n",
        "     \n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
        "        # way that we have output_words * batch_size that we want to send in into\n",
        "        # our cost function, so we need to do some reshapin.\n",
        "        # Let's also remove the start token while we're at it\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "      \n",
        "        target = target[1:].reshape(-1)\n",
        "        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "        # within a healthy range\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        # plot to tensorboard\n",
        "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "        step += 1\n",
        "\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    scheduler.step(mean_loss)\n",
        "    print(mean_loss)\n",
        "# running on entire test data takes a while\n",
        "score = bleu(test_data[1:100], model, chinese, english, device)\n",
        "print(f\"Bleu score {score * 100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0 / 1000]\n",
            "3.6758291521072386\n",
            "[Epoch 1 / 1000]\n",
            "2.7107226705551146\n",
            "[Epoch 2 / 1000]\n",
            "2.247994129896164\n",
            "[Epoch 3 / 1000]\n",
            "1.915994570016861\n",
            "[Epoch 4 / 1000]\n",
            "1.6536019275188445\n",
            "[Epoch 5 / 1000]\n",
            "1.4442564746141433\n",
            "[Epoch 6 / 1000]\n",
            "1.2686170208454133\n",
            "[Epoch 7 / 1000]\n",
            "1.1271344730854034\n",
            "[Epoch 8 / 1000]\n",
            "1.0067365480661392\n",
            "[Epoch 9 / 1000]\n",
            "0.9065022503137589\n",
            "[Epoch 10 / 1000]\n",
            "0.8271819287538529\n",
            "[Epoch 11 / 1000]\n",
            "0.756732054233551\n",
            "[Epoch 12 / 1000]\n",
            "0.6996566345095634\n",
            "[Epoch 13 / 1000]\n",
            "0.6451301282048225\n",
            "[Epoch 14 / 1000]\n",
            "0.6067051531672478\n",
            "[Epoch 15 / 1000]\n",
            "0.5773385584950447\n",
            "[Epoch 16 / 1000]\n",
            "0.5415603702664376\n",
            "[Epoch 17 / 1000]\n",
            "0.5144731886386872\n",
            "[Epoch 18 / 1000]\n",
            "0.48744395315647127\n",
            "[Epoch 19 / 1000]\n",
            "0.46902103239297865\n",
            "[Epoch 20 / 1000]\n",
            "0.4506598547697067\n",
            "[Epoch 21 / 1000]\n",
            "0.43535429254174235\n",
            "[Epoch 22 / 1000]\n",
            "0.4200000267624855\n",
            "[Epoch 23 / 1000]\n",
            "0.40910448747873307\n",
            "[Epoch 24 / 1000]\n",
            "0.3906287149190903\n",
            "[Epoch 25 / 1000]\n",
            "0.3837367487549782\n",
            "[Epoch 26 / 1000]\n",
            "0.37204250881075857\n",
            "[Epoch 27 / 1000]\n",
            "0.366415824919939\n",
            "[Epoch 28 / 1000]\n",
            "0.3543061045408249\n",
            "[Epoch 29 / 1000]\n",
            "0.34701708555221555\n",
            "[Epoch 30 / 1000]\n",
            "0.3400573419928551\n",
            "[Epoch 31 / 1000]\n",
            "0.33403278270363806\n",
            "[Epoch 32 / 1000]\n",
            "0.32522899442911146\n",
            "[Epoch 33 / 1000]\n",
            "0.3179951494038105\n",
            "[Epoch 34 / 1000]\n",
            "0.31254364049434663\n",
            "[Epoch 35 / 1000]\n",
            "0.30862401524186134\n",
            "[Epoch 36 / 1000]\n",
            "0.30479783418774603\n",
            "[Epoch 37 / 1000]\n",
            "0.3023553603589535\n",
            "[Epoch 38 / 1000]\n",
            "0.29582533541321754\n",
            "[Epoch 39 / 1000]\n",
            "0.2917717989832163\n",
            "[Epoch 40 / 1000]\n",
            "0.2838535164296627\n",
            "[Epoch 41 / 1000]\n",
            "0.28217252214252947\n",
            "[Epoch 42 / 1000]\n",
            "0.27791304457187654\n",
            "[Epoch 43 / 1000]\n",
            "0.2739759975075722\n",
            "[Epoch 44 / 1000]\n",
            "0.2688873605579138\n",
            "[Epoch 45 / 1000]\n",
            "0.26431985360383986\n",
            "[Epoch 46 / 1000]\n",
            "0.26639560669660567\n",
            "[Epoch 47 / 1000]\n",
            "0.25697102457284926\n",
            "[Epoch 48 / 1000]\n",
            "0.25656815540790556\n",
            "[Epoch 49 / 1000]\n",
            "0.25716149677336214\n",
            "[Epoch 50 / 1000]\n",
            "0.2528450214713812\n",
            "[Epoch 51 / 1000]\n",
            "0.24950383457541467\n",
            "[Epoch 52 / 1000]\n",
            "0.2466851641535759\n",
            "[Epoch 53 / 1000]\n",
            "0.2419534371048212\n",
            "[Epoch 54 / 1000]\n",
            "0.23842575572431088\n",
            "[Epoch 55 / 1000]\n",
            "0.2391242159754038\n",
            "[Epoch 56 / 1000]\n",
            "0.23261309176683426\n",
            "[Epoch 57 / 1000]\n",
            "0.2318753100335598\n",
            "[Epoch 58 / 1000]\n",
            "0.2319523807168007\n",
            "[Epoch 59 / 1000]\n",
            "0.22814541268348693\n",
            "[Epoch 60 / 1000]\n",
            "0.226169338285923\n",
            "[Epoch 61 / 1000]\n",
            "0.2236871955692768\n",
            "[Epoch 62 / 1000]\n",
            "0.22100024035573004\n",
            "[Epoch 63 / 1000]\n",
            "0.21959427934885026\n",
            "[Epoch 64 / 1000]\n",
            "0.2148989363461733\n",
            "[Epoch 65 / 1000]\n",
            "0.21479929608851672\n",
            "[Epoch 66 / 1000]\n",
            "0.21473884549736977\n",
            "[Epoch 67 / 1000]\n",
            "0.20830525946617126\n",
            "[Epoch 68 / 1000]\n",
            "0.20557620050013065\n",
            "[Epoch 69 / 1000]\n",
            "0.20878428842127322\n",
            "[Epoch 70 / 1000]\n",
            "0.2023322174102068\n",
            "[Epoch 71 / 1000]\n",
            "0.20031909988820554\n",
            "[Epoch 72 / 1000]\n",
            "0.19928196748346091\n",
            "[Epoch 73 / 1000]\n",
            "0.1992008466348052\n",
            "[Epoch 74 / 1000]\n",
            "0.19798252776265143\n",
            "[Epoch 75 / 1000]\n",
            "0.19355188653618097\n",
            "[Epoch 76 / 1000]\n",
            "0.19381279017031192\n",
            "[Epoch 77 / 1000]\n",
            "0.1911021102666855\n",
            "[Epoch 78 / 1000]\n",
            "0.18873105150461197\n",
            "[Epoch 79 / 1000]\n",
            "0.18724881237745286\n",
            "[Epoch 80 / 1000]\n",
            "0.18822610413283108\n",
            "[Epoch 81 / 1000]\n",
            "0.18430623315274716\n",
            "[Epoch 82 / 1000]\n",
            "0.182022980928421\n",
            "[Epoch 83 / 1000]\n",
            "0.18189383097738027\n",
            "[Epoch 84 / 1000]\n",
            "0.1777560680732131\n",
            "[Epoch 85 / 1000]\n",
            "0.18009228689223528\n",
            "[Epoch 86 / 1000]\n",
            "0.17423770742863418\n",
            "[Epoch 87 / 1000]\n",
            "0.17360417874902487\n",
            "[Epoch 88 / 1000]\n",
            "0.17370839133858682\n",
            "[Epoch 89 / 1000]\n",
            "0.17308024138212205\n",
            "[Epoch 90 / 1000]\n",
            "0.1706836068481207\n",
            "[Epoch 91 / 1000]\n",
            "0.1669495923370123\n",
            "[Epoch 92 / 1000]\n",
            "0.16858458361774684\n",
            "[Epoch 93 / 1000]\n",
            "0.16732077135890722\n",
            "[Epoch 94 / 1000]\n",
            "0.1645041418969631\n",
            "[Epoch 95 / 1000]\n",
            "0.1636132299080491\n",
            "[Epoch 96 / 1000]\n",
            "0.16208833698183298\n",
            "[Epoch 97 / 1000]\n",
            "0.162228261731565\n",
            "[Epoch 98 / 1000]\n",
            "0.15832687754929065\n",
            "[Epoch 99 / 1000]\n",
            "0.1601718099936843\n",
            "[Epoch 100 / 1000]\n",
            "0.156820739030838\n",
            "[Epoch 101 / 1000]\n",
            "0.15728642439842225\n",
            "[Epoch 102 / 1000]\n",
            "0.15454336284846068\n",
            "[Epoch 103 / 1000]\n",
            "0.1560709725022316\n",
            "[Epoch 104 / 1000]\n",
            "0.15534491462260486\n",
            "[Epoch 105 / 1000]\n",
            "0.1523449973128736\n",
            "[Epoch 106 / 1000]\n",
            "0.15069622544944286\n",
            "[Epoch 107 / 1000]\n",
            "0.15203451607376336\n",
            "[Epoch 108 / 1000]\n",
            "0.1492089009284973\n",
            "[Epoch 109 / 1000]\n",
            "0.14820966565608978\n",
            "[Epoch 110 / 1000]\n",
            "0.14777328249812127\n",
            "[Epoch 111 / 1000]\n",
            "0.14606727650016546\n",
            "[Epoch 112 / 1000]\n",
            "0.14701725512742997\n",
            "[Epoch 113 / 1000]\n",
            "0.14559715466946363\n",
            "[Epoch 114 / 1000]\n",
            "0.1424869776070118\n",
            "[Epoch 115 / 1000]\n",
            "0.14052487748116255\n",
            "[Epoch 116 / 1000]\n",
            "0.14141133983433246\n",
            "[Epoch 117 / 1000]\n",
            "0.1396955912038684\n",
            "[Epoch 118 / 1000]\n",
            "0.1430079954713583\n",
            "[Epoch 119 / 1000]\n",
            "0.13876850059628487\n",
            "[Epoch 120 / 1000]\n",
            "0.13673079376667738\n",
            "[Epoch 121 / 1000]\n",
            "0.13941150510311126\n",
            "[Epoch 122 / 1000]\n",
            "0.14010445587337017\n",
            "[Epoch 123 / 1000]\n",
            "0.13695813659206033\n",
            "[Epoch 124 / 1000]\n",
            "0.13344068280607463\n",
            "[Epoch 125 / 1000]\n",
            "0.13467650336399675\n",
            "[Epoch 126 / 1000]\n",
            "0.13587654215842485\n",
            "[Epoch 127 / 1000]\n",
            "0.13234811758995055\n",
            "[Epoch 128 / 1000]\n",
            "0.13295487010478974\n",
            "[Epoch 129 / 1000]\n",
            "0.1298078425899148\n",
            "[Epoch 130 / 1000]\n",
            "0.13232369919121265\n",
            "[Epoch 131 / 1000]\n",
            "0.13122560465335845\n",
            "[Epoch 132 / 1000]\n",
            "0.1315659646987915\n",
            "[Epoch 133 / 1000]\n",
            "0.1287588715851307\n",
            "[Epoch 134 / 1000]\n",
            "0.12863531175255777\n",
            "[Epoch 135 / 1000]\n",
            "0.12942522770911455\n",
            "[Epoch 136 / 1000]\n",
            "0.1262360190600157\n",
            "[Epoch 137 / 1000]\n",
            "0.12639234244823455\n",
            "[Epoch 138 / 1000]\n",
            "0.12542167872190477\n",
            "[Epoch 139 / 1000]\n",
            "0.12306629865244031\n",
            "[Epoch 140 / 1000]\n",
            "0.12745438043773175\n",
            "[Epoch 141 / 1000]\n",
            "0.12438784463331104\n",
            "[Epoch 142 / 1000]\n",
            "0.12359459492936731\n",
            "[Epoch 143 / 1000]\n",
            "0.12226700038462877\n",
            "[Epoch 144 / 1000]\n",
            "0.1219480520710349\n",
            "[Epoch 145 / 1000]\n",
            "0.12081727277487517\n",
            "[Epoch 146 / 1000]\n",
            "0.12389570926129818\n",
            "[Epoch 147 / 1000]\n",
            "0.11921534970775247\n",
            "[Epoch 148 / 1000]\n",
            "0.12091833630949259\n",
            "[Epoch 149 / 1000]\n",
            "0.1195961443260312\n",
            "[Epoch 150 / 1000]\n",
            "0.12042216416448355\n",
            "[Epoch 151 / 1000]\n",
            "0.11788970132172108\n",
            "[Epoch 152 / 1000]\n",
            "0.11908932725340128\n",
            "[Epoch 153 / 1000]\n",
            "0.11813998733833432\n",
            "[Epoch 154 / 1000]\n",
            "0.1169930815435946\n",
            "[Epoch 155 / 1000]\n",
            "0.11663921807706357\n",
            "[Epoch 156 / 1000]\n",
            "0.11811916803196072\n",
            "[Epoch 157 / 1000]\n",
            "0.11624534885585308\n",
            "[Epoch 158 / 1000]\n",
            "0.11883760859072208\n",
            "[Epoch 159 / 1000]\n",
            "0.1123676584418863\n",
            "[Epoch 160 / 1000]\n",
            "0.11135034051537514\n",
            "[Epoch 161 / 1000]\n",
            "0.1148784135878086\n",
            "[Epoch 162 / 1000]\n",
            "0.11229152243584395\n",
            "[Epoch 163 / 1000]\n",
            "0.11127127037197351\n",
            "[Epoch 164 / 1000]\n",
            "0.11379752047359944\n",
            "[Epoch 165 / 1000]\n",
            "0.11091031937487424\n",
            "[Epoch 166 / 1000]\n",
            "0.11246086423099041\n",
            "[Epoch 167 / 1000]\n",
            "0.11040710017457604\n",
            "[Epoch 168 / 1000]\n",
            "0.10967132011055947\n",
            "[Epoch 169 / 1000]\n",
            "0.10997257740795613\n",
            "[Epoch 170 / 1000]\n",
            "0.11047768839076161\n",
            "[Epoch 171 / 1000]\n",
            "0.10887232381105423\n",
            "[Epoch 172 / 1000]\n",
            "0.109845507401973\n",
            "[Epoch 173 / 1000]\n",
            "0.10770065743289887\n",
            "[Epoch 174 / 1000]\n",
            "0.10882005529850722\n",
            "[Epoch 175 / 1000]\n",
            "0.10722588808834553\n",
            "[Epoch 176 / 1000]\n",
            "0.11013983779959381\n",
            "[Epoch 177 / 1000]\n",
            "0.10890075342357158\n",
            "[Epoch 178 / 1000]\n",
            "0.10759601881727576\n",
            "[Epoch 179 / 1000]\n",
            "0.10646718521788716\n",
            "[Epoch 180 / 1000]\n",
            "0.10727612856775522\n",
            "[Epoch 181 / 1000]\n",
            "0.10628589703887702\n",
            "[Epoch 182 / 1000]\n",
            "0.10589791789650917\n",
            "[Epoch 183 / 1000]\n",
            "0.1075113629475236\n",
            "[Epoch 184 / 1000]\n",
            "0.10446670650690794\n",
            "[Epoch 185 / 1000]\n",
            "0.1042182547263801\n",
            "[Epoch 186 / 1000]\n",
            "0.10603324572741986\n",
            "[Epoch 187 / 1000]\n",
            "0.10489857648685574\n",
            "[Epoch 188 / 1000]\n",
            "0.1052360331863165\n",
            "[Epoch 189 / 1000]\n",
            "0.10308974052965642\n",
            "[Epoch 190 / 1000]\n",
            "0.10270557680353522\n",
            "[Epoch 191 / 1000]\n",
            "0.10408370493352413\n",
            "[Epoch 192 / 1000]\n",
            "0.10473687788844109\n",
            "[Epoch 193 / 1000]\n",
            "0.1033501472696662\n",
            "[Epoch 194 / 1000]\n",
            "0.10066013721749187\n",
            "[Epoch 195 / 1000]\n",
            "0.10214464220032096\n",
            "[Epoch 196 / 1000]\n",
            "0.10203015071153641\n",
            "[Epoch 197 / 1000]\n",
            "0.0980765659622848\n",
            "[Epoch 198 / 1000]\n",
            "0.09928724583610893\n",
            "[Epoch 199 / 1000]\n",
            "0.09869904220663012\n",
            "[Epoch 200 / 1000]\n",
            "0.0999557696338743\n",
            "[Epoch 201 / 1000]\n",
            "0.09856739733368158\n",
            "[Epoch 202 / 1000]\n",
            "0.10015062137693166\n",
            "[Epoch 203 / 1000]\n",
            "0.10051690884679555\n",
            "[Epoch 204 / 1000]\n",
            "0.10136791073158383\n",
            "[Epoch 205 / 1000]\n",
            "0.10062546302378178\n",
            "[Epoch 206 / 1000]\n",
            "0.09978016746602952\n",
            "[Epoch 207 / 1000]\n",
            "0.10002251541614532\n",
            "[Epoch 208 / 1000]\n",
            "Epoch   209: reducing learning rate of group 0 to 3.0000e-05.\n",
            "0.09811978701129556\n",
            "[Epoch 209 / 1000]\n",
            "0.06915638835076243\n",
            "[Epoch 210 / 1000]\n",
            "0.06062181681487709\n",
            "[Epoch 211 / 1000]\n",
            "0.05331080173887313\n",
            "[Epoch 212 / 1000]\n",
            "0.05056897215265781\n",
            "[Epoch 213 / 1000]\n",
            "0.04744348272215575\n",
            "[Epoch 214 / 1000]\n",
            "0.0439223608309403\n",
            "[Epoch 215 / 1000]\n",
            "0.04468560476601124\n",
            "[Epoch 216 / 1000]\n",
            "0.042261626719497146\n",
            "[Epoch 217 / 1000]\n",
            "0.04107294928841293\n",
            "[Epoch 218 / 1000]\n",
            "0.04067171189840883\n",
            "[Epoch 219 / 1000]\n",
            "0.039881183041725304\n",
            "[Epoch 220 / 1000]\n",
            "0.03881596440589055\n",
            "[Epoch 221 / 1000]\n",
            "0.03853452629549429\n",
            "[Epoch 222 / 1000]\n",
            "0.03894852611934766\n",
            "[Epoch 223 / 1000]\n",
            "0.038686494922731074\n",
            "[Epoch 224 / 1000]\n",
            "0.03848396147682797\n",
            "[Epoch 225 / 1000]\n",
            "0.03744960690708831\n",
            "[Epoch 226 / 1000]\n",
            "0.03724719962710515\n",
            "[Epoch 227 / 1000]\n",
            "0.03644621613668278\n",
            "[Epoch 228 / 1000]\n",
            "0.03626144543348346\n",
            "[Epoch 229 / 1000]\n",
            "0.036237432932364755\n",
            "[Epoch 230 / 1000]\n",
            "0.03625785283604637\n",
            "[Epoch 231 / 1000]\n",
            "0.03507664534379728\n",
            "[Epoch 232 / 1000]\n",
            "0.03463748780312017\n",
            "[Epoch 233 / 1000]\n",
            "0.03468219832377508\n",
            "[Epoch 234 / 1000]\n",
            "0.03523582874797285\n",
            "[Epoch 235 / 1000]\n",
            "0.03491548918094486\n",
            "[Epoch 236 / 1000]\n",
            "0.03387134423910174\n",
            "[Epoch 237 / 1000]\n",
            "0.03468732498842292\n",
            "[Epoch 238 / 1000]\n",
            "0.034107048948993905\n",
            "[Epoch 239 / 1000]\n",
            "0.03468832439323887\n",
            "[Epoch 240 / 1000]\n",
            "0.03426996689662337\n",
            "[Epoch 241 / 1000]\n",
            "0.03458823052886874\n",
            "[Epoch 242 / 1000]\n",
            "0.03401168284658342\n",
            "[Epoch 243 / 1000]\n",
            "0.033675923492526634\n",
            "[Epoch 244 / 1000]\n",
            "0.03379749727039598\n",
            "[Epoch 245 / 1000]\n",
            "0.03315352703933604\n",
            "[Epoch 246 / 1000]\n",
            "0.03329443390783854\n",
            "[Epoch 247 / 1000]\n",
            "0.03268059601937421\n",
            "[Epoch 248 / 1000]\n",
            "0.03215160971530713\n",
            "[Epoch 249 / 1000]\n",
            "0.03333265316998586\n",
            "[Epoch 250 / 1000]\n",
            "0.032448177365818995\n",
            "[Epoch 251 / 1000]\n",
            "0.03277752060745843\n",
            "[Epoch 252 / 1000]\n",
            "0.03259896499756724\n",
            "[Epoch 253 / 1000]\n",
            "0.03252229326358065\n",
            "[Epoch 254 / 1000]\n",
            "0.0332902543572709\n",
            "[Epoch 255 / 1000]\n",
            "0.03290464375633746\n",
            "[Epoch 256 / 1000]\n",
            "0.03232099517271854\n",
            "[Epoch 257 / 1000]\n",
            "0.03199925305164652\n",
            "[Epoch 258 / 1000]\n",
            "0.03259155582636595\n",
            "[Epoch 259 / 1000]\n",
            "0.031690258319489656\n",
            "[Epoch 260 / 1000]\n",
            "0.0320430992837064\n",
            "[Epoch 261 / 1000]\n",
            "0.03209455254510976\n",
            "[Epoch 262 / 1000]\n",
            "0.03149821472214535\n",
            "[Epoch 263 / 1000]\n",
            "0.0318059006605763\n",
            "[Epoch 264 / 1000]\n",
            "0.03188837828487158\n",
            "[Epoch 265 / 1000]\n",
            "0.03170769300102256\n",
            "[Epoch 266 / 1000]\n",
            "0.03162276360485703\n",
            "[Epoch 267 / 1000]\n",
            "0.031872331694816236\n",
            "[Epoch 268 / 1000]\n",
            "0.03188814457785338\n",
            "[Epoch 269 / 1000]\n",
            "0.03109440292487852\n",
            "[Epoch 270 / 1000]\n",
            "0.03192648657853715\n",
            "[Epoch 271 / 1000]\n",
            "0.03136644838948269\n",
            "[Epoch 272 / 1000]\n",
            "0.03143199108750559\n",
            "[Epoch 273 / 1000]\n",
            "0.03129600420012139\n",
            "[Epoch 274 / 1000]\n",
            "0.030593054531142114\n",
            "[Epoch 275 / 1000]\n",
            "0.031176282849395647\n",
            "[Epoch 276 / 1000]\n",
            "0.030602711189421827\n",
            "[Epoch 277 / 1000]\n",
            "0.03147123988321982\n",
            "[Epoch 278 / 1000]\n",
            "0.03068421708687674\n",
            "[Epoch 279 / 1000]\n",
            "0.030723480938118882\n",
            "[Epoch 280 / 1000]\n",
            "0.030653511129436085\n",
            "[Epoch 281 / 1000]\n",
            "0.030469900186406447\n",
            "[Epoch 282 / 1000]\n",
            "0.030555720346281304\n",
            "[Epoch 283 / 1000]\n",
            "0.030530827278038488\n",
            "[Epoch 284 / 1000]\n",
            "0.030689085243502633\n",
            "[Epoch 285 / 1000]\n",
            "0.03094880614534486\n",
            "[Epoch 286 / 1000]\n",
            "0.03089982247631997\n",
            "[Epoch 287 / 1000]\n",
            "0.03019922289182432\n",
            "[Epoch 288 / 1000]\n",
            "0.030456070738728157\n",
            "[Epoch 289 / 1000]\n",
            "0.030636508001363836\n",
            "[Epoch 290 / 1000]\n",
            "0.03068105263414327\n",
            "[Epoch 291 / 1000]\n",
            "0.029806772890035062\n",
            "[Epoch 292 / 1000]\n",
            "0.030560786604532042\n",
            "[Epoch 293 / 1000]\n",
            "0.030505110276397317\n",
            "[Epoch 294 / 1000]\n",
            "0.0306544458475546\n",
            "[Epoch 295 / 1000]\n",
            "0.030011056913761422\n",
            "[Epoch 296 / 1000]\n",
            "0.030508438274322543\n",
            "[Epoch 297 / 1000]\n",
            "0.030250668658409266\n",
            "[Epoch 298 / 1000]\n",
            "0.030370677635422907\n",
            "[Epoch 299 / 1000]\n",
            "0.02962640888337046\n",
            "[Epoch 300 / 1000]\n",
            "0.030033960516797378\n",
            "[Epoch 301 / 1000]\n",
            "0.030293047653802203\n",
            "[Epoch 302 / 1000]\n",
            "0.030969341290881858\n",
            "[Epoch 303 / 1000]\n",
            "0.030188864334253593\n",
            "[Epoch 304 / 1000]\n",
            "0.02980569864611607\n",
            "[Epoch 305 / 1000]\n",
            "0.03009329220757354\n",
            "[Epoch 306 / 1000]\n",
            "0.03013470021507237\n",
            "[Epoch 307 / 1000]\n",
            "0.02995547659206204\n",
            "[Epoch 308 / 1000]\n",
            "0.03007351932977326\n",
            "[Epoch 309 / 1000]\n",
            "0.029842508730245753\n",
            "[Epoch 310 / 1000]\n",
            "0.029318239162676035\n",
            "[Epoch 311 / 1000]\n",
            "0.02950304739479907\n",
            "[Epoch 312 / 1000]\n",
            "0.029585770584293642\n",
            "[Epoch 313 / 1000]\n",
            "0.029747217144118623\n",
            "[Epoch 314 / 1000]\n",
            "0.029463970435783266\n",
            "[Epoch 315 / 1000]\n",
            "0.029461800286313518\n",
            "[Epoch 316 / 1000]\n",
            "0.029855772982235067\n",
            "[Epoch 317 / 1000]\n",
            "0.02896152885770425\n",
            "[Epoch 318 / 1000]\n",
            "0.029378313579130916\n",
            "[Epoch 319 / 1000]\n",
            "0.02982044176632189\n",
            "[Epoch 320 / 1000]\n",
            "0.029264026181655935\n",
            "[Epoch 321 / 1000]\n",
            "0.029298459758865646\n",
            "[Epoch 322 / 1000]\n",
            "0.02911324943479849\n",
            "[Epoch 323 / 1000]\n",
            "0.0289239946086891\n",
            "[Epoch 324 / 1000]\n",
            "0.02892838709190255\n",
            "[Epoch 325 / 1000]\n",
            "0.03006786470010411\n",
            "[Epoch 326 / 1000]\n",
            "0.029384421093855053\n",
            "[Epoch 327 / 1000]\n",
            "0.029245662713306957\n",
            "[Epoch 328 / 1000]\n",
            "0.02941264295525616\n",
            "[Epoch 329 / 1000]\n",
            "0.02920548057369888\n",
            "[Epoch 330 / 1000]\n",
            "0.029763165940879844\n",
            "[Epoch 331 / 1000]\n",
            "0.029173602160997687\n",
            "[Epoch 332 / 1000]\n",
            "0.029344084603711962\n",
            "[Epoch 333 / 1000]\n",
            "0.029130360626848414\n",
            "[Epoch 334 / 1000]\n",
            "Epoch   335: reducing learning rate of group 0 to 3.0000e-06.\n",
            "0.02958280875720084\n",
            "[Epoch 335 / 1000]\n",
            "0.026755108119221403\n",
            "[Epoch 336 / 1000]\n",
            "0.02666545946081169\n",
            "[Epoch 337 / 1000]\n",
            "0.026765664036385715\n",
            "[Epoch 338 / 1000]\n",
            "0.026711629597237333\n",
            "[Epoch 339 / 1000]\n",
            "0.025902032169396988\n",
            "[Epoch 340 / 1000]\n",
            "0.026810943058575502\n",
            "[Epoch 341 / 1000]\n",
            "0.026601216934272087\n",
            "[Epoch 342 / 1000]\n",
            "0.02645149643905461\n",
            "[Epoch 343 / 1000]\n",
            "0.026765920270932837\n",
            "[Epoch 344 / 1000]\n",
            "0.026140045493608342\n",
            "[Epoch 345 / 1000]\n",
            "0.02687442131037824\n",
            "[Epoch 346 / 1000]\n",
            "0.02605649310996523\n",
            "[Epoch 347 / 1000]\n",
            "0.026197953558177688\n",
            "[Epoch 348 / 1000]\n",
            "0.026303494092891924\n",
            "[Epoch 349 / 1000]\n",
            "0.02651915446826024\n",
            "[Epoch 350 / 1000]\n",
            "Epoch   351: reducing learning rate of group 0 to 3.0000e-07.\n",
            "0.02619379552116152\n",
            "[Epoch 351 / 1000]\n",
            "0.026083836870559027\n",
            "[Epoch 352 / 1000]\n",
            "0.02599261435889639\n",
            "[Epoch 353 / 1000]\n",
            "0.02564613046112936\n",
            "[Epoch 354 / 1000]\n",
            "0.025940720638725906\n",
            "[Epoch 355 / 1000]\n",
            "0.025782293836178725\n",
            "[Epoch 356 / 1000]\n",
            "0.026224670272495133\n",
            "[Epoch 357 / 1000]\n",
            "0.02568344881833764\n",
            "[Epoch 358 / 1000]\n",
            "0.025823502682265825\n",
            "[Epoch 359 / 1000]\n",
            "0.02618642754823668\n",
            "[Epoch 360 / 1000]\n",
            "0.0265839862478897\n",
            "[Epoch 361 / 1000]\n",
            "0.026185962874093093\n",
            "[Epoch 362 / 1000]\n",
            "0.026093362424580845\n",
            "[Epoch 363 / 1000]\n",
            "0.026175920328241772\n",
            "[Epoch 364 / 1000]\n",
            "Epoch   365: reducing learning rate of group 0 to 3.0000e-08.\n",
            "0.026049337551696226\n",
            "[Epoch 365 / 1000]\n",
            "0.025919433581759223\n",
            "[Epoch 366 / 1000]\n",
            "0.02584821175452089\n",
            "[Epoch 367 / 1000]\n",
            "0.026054762932122687\n",
            "[Epoch 368 / 1000]\n",
            "0.025617068219056818\n",
            "[Epoch 369 / 1000]\n",
            "0.02610397386032855\n",
            "[Epoch 370 / 1000]\n",
            "0.025582346199225867\n",
            "[Epoch 371 / 1000]\n",
            "0.02603381015290506\n",
            "[Epoch 372 / 1000]\n",
            "0.026206801393884233\n",
            "[Epoch 373 / 1000]\n",
            "0.0257484104729956\n",
            "[Epoch 374 / 1000]\n",
            "0.02582606505835429\n",
            "[Epoch 375 / 1000]\n",
            "0.02596724950359203\n",
            "[Epoch 376 / 1000]\n",
            "0.026340207519591787\n",
            "[Epoch 377 / 1000]\n",
            "0.02581628287839703\n",
            "[Epoch 378 / 1000]\n",
            "0.025807800373178907\n",
            "[Epoch 379 / 1000]\n",
            "0.026011741758673452\n",
            "[Epoch 380 / 1000]\n",
            "0.025460723213327582\n",
            "[Epoch 381 / 1000]\n",
            "0.02602936946437694\n",
            "[Epoch 382 / 1000]\n",
            "0.02548779500834644\n",
            "[Epoch 383 / 1000]\n",
            "0.02550015658326447\n",
            "[Epoch 384 / 1000]\n",
            "0.02642793966445606\n",
            "[Epoch 385 / 1000]\n",
            "0.025475719460169785\n",
            "[Epoch 386 / 1000]\n",
            "0.025388962736178654\n",
            "[Epoch 387 / 1000]\n",
            "0.026213042244198733\n",
            "[Epoch 388 / 1000]\n",
            "0.02575807119649835\n",
            "[Epoch 389 / 1000]\n",
            "0.025676908766967245\n",
            "[Epoch 390 / 1000]\n",
            "0.02604913820052752\n",
            "[Epoch 391 / 1000]\n",
            "0.025522842290985864\n",
            "[Epoch 392 / 1000]\n",
            "0.025903442974260543\n",
            "[Epoch 393 / 1000]\n",
            "0.02590480216278229\n",
            "[Epoch 394 / 1000]\n",
            "0.026382174867729192\n",
            "[Epoch 395 / 1000]\n",
            "0.026233039216487668\n",
            "[Epoch 396 / 1000]\n",
            "0.02632594341353979\n",
            "[Epoch 397 / 1000]\n",
            "Epoch   398: reducing learning rate of group 0 to 3.0000e-09.\n",
            "0.02565197819378227\n",
            "[Epoch 398 / 1000]\n",
            "0.02596555242955219\n",
            "[Epoch 399 / 1000]\n",
            "0.026218778657435906\n",
            "[Epoch 400 / 1000]\n",
            "0.025732424103538504\n",
            "[Epoch 401 / 1000]\n",
            "0.02654933368077036\n",
            "[Epoch 402 / 1000]\n",
            "0.025808499146136456\n",
            "[Epoch 403 / 1000]\n",
            "0.02560270198993385\n",
            "[Epoch 404 / 1000]\n",
            "0.026705181109020485\n",
            "[Epoch 405 / 1000]\n",
            "0.025580899243301248\n",
            "[Epoch 406 / 1000]\n",
            "0.026411284110159614\n",
            "[Epoch 407 / 1000]\n",
            "0.0264566309080692\n",
            "[Epoch 408 / 1000]\n",
            "0.026119948638021015\n",
            "[Epoch 409 / 1000]\n",
            "0.02580961190292146\n",
            "[Epoch 410 / 1000]\n",
            "0.025474897459847854\n",
            "[Epoch 411 / 1000]\n",
            "0.024944274955778382\n",
            "[Epoch 412 / 1000]\n",
            "0.02583952732558828\n",
            "[Epoch 413 / 1000]\n",
            "0.026097961095627398\n",
            "[Epoch 414 / 1000]\n",
            "0.025913640905404465\n",
            "[Epoch 415 / 1000]\n",
            "0.025861631738662252\n",
            "[Epoch 416 / 1000]\n",
            "0.02593916226841975\n",
            "[Epoch 417 / 1000]\n",
            "0.02627138266485417\n",
            "[Epoch 418 / 1000]\n",
            "0.026324363291903866\n",
            "[Epoch 419 / 1000]\n",
            "0.02587440551770851\n",
            "[Epoch 420 / 1000]\n",
            "0.025735627061279957\n",
            "[Epoch 421 / 1000]\n",
            "0.025989420946047177\n",
            "[Epoch 422 / 1000]\n",
            "0.0263224935894832\n",
            "[Epoch 423 / 1000]\n",
            "0.025718333477620037\n",
            "[Epoch 424 / 1000]\n",
            "0.02526752455031965\n",
            "[Epoch 425 / 1000]\n",
            "0.0263299003894208\n",
            "[Epoch 426 / 1000]\n",
            "0.025771043214364908\n",
            "[Epoch 427 / 1000]\n",
            "0.026253261302947067\n",
            "[Epoch 428 / 1000]\n",
            "0.025698094200808555\n",
            "[Epoch 429 / 1000]\n",
            "0.026563435392279643\n",
            "[Epoch 430 / 1000]\n",
            "0.02570713867712766\n",
            "[Epoch 431 / 1000]\n",
            "0.02558273621625267\n",
            "[Epoch 432 / 1000]\n",
            "0.02657039550319314\n",
            "[Epoch 433 / 1000]\n",
            "0.02579293388815131\n",
            "[Epoch 434 / 1000]\n",
            "0.02551009523135144\n",
            "[Epoch 435 / 1000]\n",
            "0.025965763841115402\n",
            "[Epoch 436 / 1000]\n",
            "0.025392563073313796\n",
            "[Epoch 437 / 1000]\n",
            "0.025556084930547514\n",
            "[Epoch 438 / 1000]\n",
            "0.025983068619156256\n",
            "[Epoch 439 / 1000]\n",
            "0.026100453496095723\n",
            "[Epoch 440 / 1000]\n",
            "0.025962693786248565\n",
            "[Epoch 441 / 1000]\n",
            "0.025954203100525774\n",
            "[Epoch 442 / 1000]\n",
            "0.025961563542834482\n",
            "[Epoch 443 / 1000]\n",
            "0.02605066908034496\n",
            "[Epoch 444 / 1000]\n",
            "0.025872729963622988\n",
            "[Epoch 445 / 1000]\n",
            "0.0258093848786084\n",
            "[Epoch 446 / 1000]\n",
            "0.026163324780529365\n",
            "[Epoch 447 / 1000]\n",
            "0.025740076550107915\n",
            "[Epoch 448 / 1000]\n",
            "0.02617845866223797\n",
            "[Epoch 449 / 1000]\n",
            "0.026121637828007808\n",
            "[Epoch 450 / 1000]\n",
            "0.026030969252460637\n",
            "[Epoch 451 / 1000]\n",
            "0.026309170919004827\n",
            "[Epoch 452 / 1000]\n",
            "0.025963861383963376\n",
            "[Epoch 453 / 1000]\n",
            "0.026567016228917054\n",
            "[Epoch 454 / 1000]\n",
            "0.025723554493975825\n",
            "[Epoch 455 / 1000]\n",
            "0.02612313680083025\n",
            "[Epoch 456 / 1000]\n",
            "0.025700401722511743\n",
            "[Epoch 457 / 1000]\n",
            "0.026012631227436942\n",
            "[Epoch 458 / 1000]\n",
            "0.026043696992855983\n",
            "[Epoch 459 / 1000]\n",
            "0.02572584251512308\n",
            "[Epoch 460 / 1000]\n",
            "0.025944564601581076\n",
            "[Epoch 461 / 1000]\n",
            "0.025787632377177943\n",
            "[Epoch 462 / 1000]\n",
            "0.02630389088729862\n",
            "[Epoch 463 / 1000]\n",
            "0.025877559218497482\n",
            "[Epoch 464 / 1000]\n",
            "0.025752911585150286\n",
            "[Epoch 465 / 1000]\n",
            "0.025750007876660676\n",
            "[Epoch 466 / 1000]\n",
            "0.026155936707218642\n",
            "[Epoch 467 / 1000]\n",
            "0.025623518381675238\n",
            "[Epoch 468 / 1000]\n",
            "0.02606760269275401\n",
            "[Epoch 469 / 1000]\n",
            "0.025960273899778258\n",
            "[Epoch 470 / 1000]\n",
            "0.025569104211230296\n",
            "[Epoch 471 / 1000]\n",
            "0.026249126582290047\n",
            "[Epoch 472 / 1000]\n",
            "0.02552689013653435\n",
            "[Epoch 473 / 1000]\n",
            "0.025689971132203938\n",
            "[Epoch 474 / 1000]\n",
            "0.0258508468111977\n",
            "[Epoch 475 / 1000]\n",
            "0.026335479373927228\n",
            "[Epoch 476 / 1000]\n",
            "0.025610519550042225\n",
            "[Epoch 477 / 1000]\n",
            "0.02647097336302977\n",
            "[Epoch 478 / 1000]\n",
            "0.025592427700292318\n",
            "[Epoch 479 / 1000]\n",
            "0.026125118306837974\n",
            "[Epoch 480 / 1000]\n",
            "0.02593809731444344\n",
            "[Epoch 481 / 1000]\n",
            "0.02582834147842368\n",
            "[Epoch 482 / 1000]\n",
            "0.025943866453599185\n",
            "[Epoch 483 / 1000]\n",
            "0.025916602382960265\n",
            "[Epoch 484 / 1000]\n",
            "0.02646219055214897\n",
            "[Epoch 485 / 1000]\n",
            "0.026059400141763035\n",
            "[Epoch 486 / 1000]\n",
            "0.025634177866973914\n",
            "[Epoch 487 / 1000]\n",
            "0.02577515598013997\n",
            "[Epoch 488 / 1000]\n",
            "0.026124646208656486\n",
            "[Epoch 489 / 1000]\n",
            "0.02630303281144006\n",
            "[Epoch 490 / 1000]\n",
            "0.025684668663947377\n",
            "[Epoch 491 / 1000]\n",
            "0.0263126272020163\n",
            "[Epoch 492 / 1000]\n",
            "0.025644086625892668\n",
            "[Epoch 493 / 1000]\n",
            "0.025648936775280164\n",
            "[Epoch 494 / 1000]\n",
            "0.026402815254055894\n",
            "[Epoch 495 / 1000]\n",
            "0.025507818145677447\n",
            "[Epoch 496 / 1000]\n",
            "0.02621066658035852\n",
            "[Epoch 497 / 1000]\n",
            "0.026124590966792312\n",
            "[Epoch 498 / 1000]\n",
            "0.02582352725195233\n",
            "[Epoch 499 / 1000]\n",
            "0.02580574857548345\n",
            "[Epoch 500 / 1000]\n",
            "0.02640527693304466\n",
            "[Epoch 501 / 1000]\n",
            "0.025811072910146324\n",
            "[Epoch 502 / 1000]\n",
            "0.02568347206118051\n",
            "[Epoch 503 / 1000]\n",
            "0.025959958461578937\n",
            "[Epoch 504 / 1000]\n",
            "0.025698751814779826\n",
            "[Epoch 505 / 1000]\n",
            "0.026051147544872946\n",
            "[Epoch 506 / 1000]\n",
            "0.02601316987595055\n",
            "[Epoch 507 / 1000]\n",
            "0.025851688224181997\n",
            "[Epoch 508 / 1000]\n",
            "0.02605475700745592\n",
            "[Epoch 509 / 1000]\n",
            "0.0256951266186079\n",
            "[Epoch 510 / 1000]\n",
            "0.02572152978120721\n",
            "[Epoch 511 / 1000]\n",
            "0.025742366452584976\n",
            "[Epoch 512 / 1000]\n",
            "0.026061056804726833\n",
            "[Epoch 513 / 1000]\n",
            "0.025667647252528694\n",
            "[Epoch 514 / 1000]\n",
            "0.025686802035721483\n",
            "[Epoch 515 / 1000]\n",
            "0.025779813879169524\n",
            "[Epoch 516 / 1000]\n",
            "0.025667812341591344\n",
            "[Epoch 517 / 1000]\n",
            "0.026410200511891163\n",
            "[Epoch 518 / 1000]\n",
            "0.025355027758982032\n",
            "[Epoch 519 / 1000]\n",
            "0.02613134349125903\n",
            "[Epoch 520 / 1000]\n",
            "0.0258112267118413\n",
            "[Epoch 521 / 1000]\n",
            "0.026234084566356616\n",
            "[Epoch 522 / 1000]\n",
            "0.026312496074475347\n",
            "[Epoch 523 / 1000]\n",
            "0.026213591304956936\n",
            "[Epoch 524 / 1000]\n",
            "0.02604072780988645\n",
            "[Epoch 525 / 1000]\n",
            "0.025780092462548056\n",
            "[Epoch 526 / 1000]\n",
            "0.025719901954871602\n",
            "[Epoch 527 / 1000]\n",
            "0.025865522719104774\n",
            "[Epoch 528 / 1000]\n",
            "0.026263110997271723\n",
            "[Epoch 529 / 1000]\n",
            "0.026000650269910695\n",
            "[Epoch 530 / 1000]\n",
            "0.025993069605901836\n",
            "[Epoch 531 / 1000]\n",
            "0.026108174733701164\n",
            "[Epoch 532 / 1000]\n",
            "0.025072375795803965\n",
            "[Epoch 533 / 1000]\n",
            "0.025276400693750475\n",
            "[Epoch 534 / 1000]\n",
            "0.02625033238658216\n",
            "[Epoch 535 / 1000]\n",
            "0.02542714985483326\n",
            "[Epoch 536 / 1000]\n",
            "0.025945465484343004\n",
            "[Epoch 537 / 1000]\n",
            "0.02608326892799232\n",
            "[Epoch 538 / 1000]\n",
            "0.02518764959461987\n",
            "[Epoch 539 / 1000]\n",
            "0.026104230988537893\n",
            "[Epoch 540 / 1000]\n",
            "0.02592896993330214\n",
            "[Epoch 541 / 1000]\n",
            "0.02577828140699421\n",
            "[Epoch 542 / 1000]\n",
            "0.025883428014931268\n",
            "[Epoch 543 / 1000]\n",
            "0.025936430603032933\n",
            "[Epoch 544 / 1000]\n",
            "0.025726503239013255\n",
            "[Epoch 545 / 1000]\n",
            "0.026258538555237465\n",
            "[Epoch 546 / 1000]\n",
            "0.025828511226456612\n",
            "[Epoch 547 / 1000]\n",
            "0.025903830386057963\n",
            "[Epoch 548 / 1000]\n",
            "0.026072479667840525\n",
            "[Epoch 549 / 1000]\n",
            "0.026011493661178973\n",
            "[Epoch 550 / 1000]\n",
            "0.025784629407513422\n",
            "[Epoch 551 / 1000]\n",
            "0.025548748319968582\n",
            "[Epoch 552 / 1000]\n",
            "0.026159977087314473\n",
            "[Epoch 553 / 1000]\n",
            "0.025824092261726037\n",
            "[Epoch 554 / 1000]\n",
            "0.025599123918800613\n",
            "[Epoch 555 / 1000]\n",
            "0.02601250768371392\n",
            "[Epoch 556 / 1000]\n",
            "0.026143124501861167\n",
            "[Epoch 557 / 1000]\n",
            "0.025828617427730932\n",
            "[Epoch 558 / 1000]\n",
            "0.026535009522456677\n",
            "[Epoch 559 / 1000]\n",
            "0.026304930441314354\n",
            "[Epoch 560 / 1000]\n",
            "0.02559467937730369\n",
            "[Epoch 561 / 1000]\n",
            "0.025410239208256826\n",
            "[Epoch 562 / 1000]\n",
            "0.025266290839936117\n",
            "[Epoch 563 / 1000]\n",
            "0.026179265527869575\n",
            "[Epoch 564 / 1000]\n",
            "0.02575916394055821\n",
            "[Epoch 565 / 1000]\n",
            "0.025595702756312677\n",
            "[Epoch 566 / 1000]\n",
            "0.0259245134384837\n",
            "[Epoch 567 / 1000]\n",
            "0.026026441741851158\n",
            "[Epoch 568 / 1000]\n",
            "0.025878289742278866\n",
            "[Epoch 569 / 1000]\n",
            "0.026402577295259105\n",
            "[Epoch 570 / 1000]\n",
            "0.025821947276883292\n",
            "[Epoch 571 / 1000]\n",
            "0.02537312699609902\n",
            "[Epoch 572 / 1000]\n",
            "0.025780829544528386\n",
            "[Epoch 573 / 1000]\n",
            "0.02573062133725034\n",
            "[Epoch 574 / 1000]\n",
            "0.026048638832871802\n",
            "[Epoch 575 / 1000]\n",
            "0.02574807060288731\n",
            "[Epoch 576 / 1000]\n",
            "0.025597340615873692\n",
            "[Epoch 577 / 1000]\n",
            "0.025887477220385334\n",
            "[Epoch 578 / 1000]\n",
            "0.02613882850226946\n",
            "[Epoch 579 / 1000]\n",
            "0.025924098232295363\n",
            "[Epoch 580 / 1000]\n",
            "0.026262791360844858\n",
            "[Epoch 581 / 1000]\n",
            "0.025963842404424214\n",
            "[Epoch 582 / 1000]\n",
            "0.02637611787789501\n",
            "[Epoch 583 / 1000]\n",
            "0.026031099156476556\n",
            "[Epoch 584 / 1000]\n",
            "0.02620546194643248\n",
            "[Epoch 585 / 1000]\n",
            "0.02600753545877524\n",
            "[Epoch 586 / 1000]\n",
            "0.025462963934754952\n",
            "[Epoch 587 / 1000]\n",
            "0.025541020150936674\n",
            "[Epoch 588 / 1000]\n",
            "0.025404965024848934\n",
            "[Epoch 589 / 1000]\n",
            "0.02606564710935345\n",
            "[Epoch 590 / 1000]\n",
            "0.026363824313506486\n",
            "[Epoch 591 / 1000]\n",
            "0.025667088467744178\n",
            "[Epoch 592 / 1000]\n",
            "0.02616341395134805\n",
            "[Epoch 593 / 1000]\n",
            "0.025765310589340517\n",
            "[Epoch 594 / 1000]\n",
            "0.025716727981867735\n",
            "[Epoch 595 / 1000]\n",
            "0.026013961026794277\n",
            "[Epoch 596 / 1000]\n",
            "0.026097109252761583\n",
            "[Epoch 597 / 1000]\n",
            "0.026026848543435337\n",
            "[Epoch 598 / 1000]\n",
            "0.026036011266871356\n",
            "[Epoch 599 / 1000]\n",
            "0.026131420885853004\n",
            "[Epoch 600 / 1000]\n",
            "0.0259675121428445\n",
            "[Epoch 601 / 1000]\n",
            "0.025914052066393196\n",
            "[Epoch 602 / 1000]\n",
            "0.02636001759232022\n",
            "[Epoch 603 / 1000]\n",
            "0.025887358270061667\n",
            "[Epoch 604 / 1000]\n",
            "0.026159506907104516\n",
            "[Epoch 605 / 1000]\n",
            "0.025887298502551857\n",
            "[Epoch 606 / 1000]\n",
            "0.025852046865795272\n",
            "[Epoch 607 / 1000]\n",
            "0.02604504738561809\n",
            "[Epoch 608 / 1000]\n",
            "0.026146696552750655\n",
            "[Epoch 609 / 1000]\n",
            "0.026230557916802354\n",
            "[Epoch 610 / 1000]\n",
            "0.026412338083027863\n",
            "[Epoch 611 / 1000]\n",
            "0.025925218415737618\n",
            "[Epoch 612 / 1000]\n",
            "0.025500902370084077\n",
            "[Epoch 613 / 1000]\n",
            "0.026051773514715024\n",
            "[Epoch 614 / 1000]\n",
            "0.025959789724845906\n",
            "[Epoch 615 / 1000]\n",
            "0.025930102835583967\n",
            "[Epoch 616 / 1000]\n",
            "0.02574641099839937\n",
            "[Epoch 617 / 1000]\n",
            "0.026292561771231703\n",
            "[Epoch 618 / 1000]\n",
            "0.025788253667822572\n",
            "[Epoch 619 / 1000]\n",
            "0.025624948699376544\n",
            "[Epoch 620 / 1000]\n",
            "0.025821207857807168\n",
            "[Epoch 621 / 1000]\n",
            "0.02622413774864981\n",
            "[Epoch 622 / 1000]\n",
            "0.02575855149788549\n",
            "[Epoch 623 / 1000]\n",
            "0.02589304610696854\n",
            "[Epoch 624 / 1000]\n",
            "0.025967331478372217\n",
            "[Epoch 625 / 1000]\n",
            "0.025929472690098918\n",
            "[Epoch 626 / 1000]\n",
            "0.026113983756891685\n",
            "[Epoch 627 / 1000]\n",
            "0.02582932540727779\n",
            "[Epoch 628 / 1000]\n",
            "0.02617877043102635\n",
            "[Epoch 629 / 1000]\n",
            "0.02589764523069607\n",
            "[Epoch 630 / 1000]\n",
            "0.026639500959950966\n",
            "[Epoch 631 / 1000]\n",
            "0.026143084055220242\n",
            "[Epoch 632 / 1000]\n",
            "0.0258011481052381\n",
            "[Epoch 633 / 1000]\n",
            "0.026182208703365177\n",
            "[Epoch 634 / 1000]\n",
            "0.02593987784557976\n",
            "[Epoch 635 / 1000]\n",
            "0.025918652423715684\n",
            "[Epoch 636 / 1000]\n",
            "0.02580881130718626\n",
            "[Epoch 637 / 1000]\n",
            "0.026025618300191126\n",
            "[Epoch 638 / 1000]\n",
            "0.026084321226458998\n",
            "[Epoch 639 / 1000]\n",
            "0.025767691581335385\n",
            "[Epoch 640 / 1000]\n",
            "0.026092810944770464\n",
            "[Epoch 641 / 1000]\n",
            "0.025852611985639668\n",
            "[Epoch 642 / 1000]\n",
            "0.0258058658161317\n",
            "[Epoch 643 / 1000]\n",
            "0.02558078260347247\n",
            "[Epoch 644 / 1000]\n",
            "0.025796152604772943\n",
            "[Epoch 645 / 1000]\n",
            "0.025862984190811403\n",
            "[Epoch 646 / 1000]\n",
            "0.025548506805149373\n",
            "[Epoch 647 / 1000]\n",
            "0.025990358920651487\n",
            "[Epoch 648 / 1000]\n",
            "0.025977105967118407\n",
            "[Epoch 649 / 1000]\n",
            "0.026162395688937978\n",
            "[Epoch 650 / 1000]\n",
            "0.025559165170125197\n",
            "[Epoch 651 / 1000]\n",
            "0.025619814655568916\n",
            "[Epoch 652 / 1000]\n",
            "0.026167623003129848\n",
            "[Epoch 653 / 1000]\n",
            "0.02624684969068039\n",
            "[Epoch 654 / 1000]\n",
            "0.026069643883558456\n",
            "[Epoch 655 / 1000]\n",
            "0.025832629068288954\n",
            "[Epoch 656 / 1000]\n",
            "0.025743191480403767\n",
            "[Epoch 657 / 1000]\n",
            "0.026126608343271074\n",
            "[Epoch 658 / 1000]\n",
            "0.025748779137269594\n",
            "[Epoch 659 / 1000]\n",
            "0.025916336547757966\n",
            "[Epoch 660 / 1000]\n",
            "0.026157761199399828\n",
            "[Epoch 661 / 1000]\n",
            "0.025504163991077803\n",
            "[Epoch 662 / 1000]\n",
            "0.026214376100862864\n",
            "[Epoch 663 / 1000]\n",
            "0.025666049740917515\n",
            "[Epoch 664 / 1000]\n",
            "0.025998838954168605\n",
            "[Epoch 665 / 1000]\n",
            "0.025969385933189187\n",
            "[Epoch 666 / 1000]\n",
            "0.02590944943856448\n",
            "[Epoch 667 / 1000]\n",
            "0.025522338261362167\n",
            "[Epoch 668 / 1000]\n",
            "0.025886791691882537\n",
            "[Epoch 669 / 1000]\n",
            "0.025906989329436327\n",
            "[Epoch 670 / 1000]\n",
            "0.025668479913962074\n",
            "[Epoch 671 / 1000]\n",
            "0.026107152219978162\n",
            "[Epoch 672 / 1000]\n",
            "0.025318241236032917\n",
            "[Epoch 673 / 1000]\n",
            "0.025758072160126176\n",
            "[Epoch 674 / 1000]\n",
            "0.026161691096262076\n",
            "[Epoch 675 / 1000]\n",
            "0.025745833810506154\n",
            "[Epoch 676 / 1000]\n",
            "0.02565343267749995\n",
            "[Epoch 677 / 1000]\n",
            "0.02565546749686473\n",
            "[Epoch 678 / 1000]\n",
            "0.026264781050151214\n",
            "[Epoch 679 / 1000]\n",
            "0.025920531091862358\n",
            "[Epoch 680 / 1000]\n",
            "0.026102668193518183\n",
            "[Epoch 681 / 1000]\n",
            "0.025842384062765632\n",
            "[Epoch 682 / 1000]\n",
            "0.025867267934721893\n",
            "[Epoch 683 / 1000]\n",
            "0.025901422291470227\n",
            "[Epoch 684 / 1000]\n",
            "0.026102868729503827\n",
            "[Epoch 685 / 1000]\n",
            "0.025522669786820187\n",
            "[Epoch 686 / 1000]\n",
            "0.026345782753429376\n",
            "[Epoch 687 / 1000]\n",
            "0.025612665070046205\n",
            "[Epoch 688 / 1000]\n",
            "0.026536625876440668\n",
            "[Epoch 689 / 1000]\n",
            "0.025808102894690817\n",
            "[Epoch 690 / 1000]\n",
            "0.026150586538540663\n",
            "[Epoch 691 / 1000]\n",
            "0.026162841015728192\n",
            "[Epoch 692 / 1000]\n",
            "0.025902983056148514\n",
            "[Epoch 693 / 1000]\n",
            "0.025784697164141106\n",
            "[Epoch 694 / 1000]\n",
            "0.025921175037277863\n",
            "[Epoch 695 / 1000]\n",
            "0.02575661689520348\n",
            "[Epoch 696 / 1000]\n",
            "0.02574592542881146\n",
            "[Epoch 697 / 1000]\n",
            "0.025095588083495386\n",
            "[Epoch 698 / 1000]\n",
            "0.025800691744545474\n",
            "[Epoch 699 / 1000]\n",
            "0.025921789148647803\n",
            "[Epoch 700 / 1000]\n",
            "0.025945405573118478\n",
            "[Epoch 701 / 1000]\n",
            "0.02624687711225124\n",
            "[Epoch 702 / 1000]\n",
            "0.02624987203767523\n",
            "[Epoch 703 / 1000]\n",
            "0.025700438778847454\n",
            "[Epoch 704 / 1000]\n",
            "0.026129237272776662\n",
            "[Epoch 705 / 1000]\n",
            "0.025907159141032025\n",
            "[Epoch 706 / 1000]\n",
            "0.025859915450331755\n",
            "[Epoch 707 / 1000]\n",
            "0.02611861188506009\n",
            "[Epoch 708 / 1000]\n",
            "0.025664471792988478\n",
            "[Epoch 709 / 1000]\n",
            "0.025901969683123752\n",
            "[Epoch 710 / 1000]\n",
            "0.025668814141245094\n",
            "[Epoch 711 / 1000]\n",
            "0.02548786817159271\n",
            "[Epoch 712 / 1000]\n",
            "0.025891295923152938\n",
            "[Epoch 713 / 1000]\n",
            "0.025615674452506936\n",
            "[Epoch 714 / 1000]\n",
            "0.026012961085361896\n",
            "[Epoch 715 / 1000]\n",
            "0.025535585281730163\n",
            "[Epoch 716 / 1000]\n",
            "0.025662979636806995\n",
            "[Epoch 717 / 1000]\n",
            "0.02645043670356972\n",
            "[Epoch 718 / 1000]\n",
            "0.02575529292889405\n",
            "[Epoch 719 / 1000]\n",
            "0.025885052347905002\n",
            "[Epoch 720 / 1000]\n",
            "0.026669664466171524\n",
            "[Epoch 721 / 1000]\n",
            "0.026095005064911674\n",
            "[Epoch 722 / 1000]\n",
            "0.026442416546400636\n",
            "[Epoch 723 / 1000]\n",
            "0.025612999125325588\n",
            "[Epoch 724 / 1000]\n",
            "0.025534856793878135\n",
            "[Epoch 725 / 1000]\n",
            "0.026232906467397697\n",
            "[Epoch 726 / 1000]\n",
            "0.026338950072298757\n",
            "[Epoch 727 / 1000]\n",
            "0.02649351354292594\n",
            "[Epoch 728 / 1000]\n",
            "0.02617160278838128\n",
            "[Epoch 729 / 1000]\n",
            "0.025875398413743825\n",
            "[Epoch 730 / 1000]\n",
            "0.02564360138832126\n",
            "[Epoch 731 / 1000]\n",
            "0.026290272758575157\n",
            "[Epoch 732 / 1000]\n",
            "0.02580970968375914\n",
            "[Epoch 733 / 1000]\n",
            "0.025910374937695452\n",
            "[Epoch 734 / 1000]\n",
            "0.0259466696108575\n",
            "[Epoch 735 / 1000]\n",
            "0.02622670990595361\n",
            "[Epoch 736 / 1000]\n",
            "0.026289001162280327\n",
            "[Epoch 737 / 1000]\n",
            "0.025634098397044\n",
            "[Epoch 738 / 1000]\n",
            "0.025559463601210156\n",
            "[Epoch 739 / 1000]\n",
            "0.026279191307199655\n",
            "[Epoch 740 / 1000]\n",
            "0.026219661572773476\n",
            "[Epoch 741 / 1000]\n",
            "0.02583576240541879\n",
            "[Epoch 742 / 1000]\n",
            "0.02602813576266635\n",
            "[Epoch 743 / 1000]\n",
            "0.025523576127015987\n",
            "[Epoch 744 / 1000]\n",
            "0.02654630105069373\n",
            "[Epoch 745 / 1000]\n",
            "0.025718115009716713\n",
            "[Epoch 746 / 1000]\n",
            "0.025949591673677788\n",
            "[Epoch 747 / 1000]\n",
            "0.025829373946064153\n",
            "[Epoch 748 / 1000]\n",
            "0.025767228596960193\n",
            "[Epoch 749 / 1000]\n",
            "0.026207181205565575\n",
            "[Epoch 750 / 1000]\n",
            "0.026118343707639723\n",
            "[Epoch 751 / 1000]\n",
            "0.025404176357085816\n",
            "[Epoch 752 / 1000]\n",
            "0.025620078649255446\n",
            "[Epoch 753 / 1000]\n",
            "0.025931483964552172\n",
            "[Epoch 754 / 1000]\n",
            "0.025969559711753392\n",
            "[Epoch 755 / 1000]\n",
            "0.02617761980299838\n",
            "[Epoch 756 / 1000]\n",
            "0.025796388454386032\n",
            "[Epoch 757 / 1000]\n",
            "0.02558932760776952\n",
            "[Epoch 758 / 1000]\n",
            "0.025741712732124142\n",
            "[Epoch 759 / 1000]\n",
            "0.025744963512348475\n",
            "[Epoch 760 / 1000]\n",
            "0.025849888335564175\n",
            "[Epoch 761 / 1000]\n",
            "0.025745864000404253\n",
            "[Epoch 762 / 1000]\n",
            "0.025401842664403377\n",
            "[Epoch 763 / 1000]\n",
            "0.026304438568477053\n",
            "[Epoch 764 / 1000]\n",
            "0.02599252932606032\n",
            "[Epoch 765 / 1000]\n",
            "0.026056911183870397\n",
            "[Epoch 766 / 1000]\n",
            "0.025929713076620828\n",
            "[Epoch 767 / 1000]\n",
            "0.0261649077963084\n",
            "[Epoch 768 / 1000]\n",
            "0.02596497965522576\n",
            "[Epoch 769 / 1000]\n",
            "0.025985099690326023\n",
            "[Epoch 770 / 1000]\n",
            "0.026509915091097354\n",
            "[Epoch 771 / 1000]\n",
            "0.02613206371071283\n",
            "[Epoch 772 / 1000]\n",
            "0.025600425215205176\n",
            "[Epoch 773 / 1000]\n",
            "0.025110249305784236\n",
            "[Epoch 774 / 1000]\n",
            "0.026445941109675915\n",
            "[Epoch 775 / 1000]\n",
            "0.025964631882146934\n",
            "[Epoch 776 / 1000]\n",
            "0.025457622288551647\n",
            "[Epoch 777 / 1000]\n",
            "0.025894677256816066\n",
            "[Epoch 778 / 1000]\n",
            "0.026011290616530458\n",
            "[Epoch 779 / 1000]\n",
            "0.025339854626043233\n",
            "[Epoch 780 / 1000]\n",
            "0.025935229566530323\n",
            "[Epoch 781 / 1000]\n",
            "0.02550582709320588\n",
            "[Epoch 782 / 1000]\n",
            "0.025752816804335454\n",
            "[Epoch 783 / 1000]\n",
            "0.026104024632950314\n",
            "[Epoch 784 / 1000]\n",
            "0.025481250557349996\n",
            "[Epoch 785 / 1000]\n",
            "0.025861196979065425\n",
            "[Epoch 786 / 1000]\n",
            "0.026113565619685686\n",
            "[Epoch 787 / 1000]\n",
            "0.026328247472061774\n",
            "[Epoch 788 / 1000]\n",
            "0.025539555260445924\n",
            "[Epoch 789 / 1000]\n",
            "0.025188354737649205\n",
            "[Epoch 790 / 1000]\n",
            "0.026050277693779208\n",
            "[Epoch 791 / 1000]\n",
            "0.025554595490684732\n",
            "[Epoch 792 / 1000]\n",
            "0.025584538969793357\n",
            "[Epoch 793 / 1000]\n",
            "0.026620907154341695\n",
            "[Epoch 794 / 1000]\n",
            "0.02615028577414341\n",
            "[Epoch 795 / 1000]\n",
            "0.025786360420868733\n",
            "[Epoch 796 / 1000]\n",
            "0.025804573738249018\n",
            "[Epoch 797 / 1000]\n",
            "0.025659858741913923\n",
            "[Epoch 798 / 1000]\n",
            "0.02667166254617041\n",
            "[Epoch 799 / 1000]\n",
            "0.025726270438812208\n",
            "[Epoch 800 / 1000]\n",
            "0.025664143676694947\n",
            "[Epoch 801 / 1000]\n",
            "0.02609090943034971\n",
            "[Epoch 802 / 1000]\n",
            "0.025972605809685773\n",
            "[Epoch 803 / 1000]\n",
            "0.02625563838443486\n",
            "[Epoch 804 / 1000]\n",
            "0.025732168537448162\n",
            "[Epoch 805 / 1000]\n",
            "0.025825211452844086\n",
            "[Epoch 806 / 1000]\n",
            "0.026383369079383556\n",
            "[Epoch 807 / 1000]\n",
            "0.02668854112306144\n",
            "[Epoch 808 / 1000]\n",
            "0.025695544719696047\n",
            "[Epoch 809 / 1000]\n",
            "0.026136914138507564\n",
            "[Epoch 810 / 1000]\n",
            "0.02556559749774169\n",
            "[Epoch 811 / 1000]\n",
            "0.026247481198457535\n",
            "[Epoch 812 / 1000]\n",
            "0.025848652060201857\n",
            "[Epoch 813 / 1000]\n",
            "0.025569736127450598\n",
            "[Epoch 814 / 1000]\n",
            "0.02569115438021254\n",
            "[Epoch 815 / 1000]\n",
            "0.025806115291488824\n",
            "[Epoch 816 / 1000]\n",
            "0.026030027321656234\n",
            "[Epoch 817 / 1000]\n",
            "0.02590562196576502\n",
            "[Epoch 818 / 1000]\n",
            "0.02610749738366576\n",
            "[Epoch 819 / 1000]\n",
            "0.0258983907434158\n",
            "[Epoch 820 / 1000]\n",
            "0.026129987810272724\n",
            "[Epoch 821 / 1000]\n",
            "0.025924773776554503\n",
            "[Epoch 822 / 1000]\n",
            "0.026195999368210324\n",
            "[Epoch 823 / 1000]\n",
            "0.02565175071603153\n",
            "[Epoch 824 / 1000]\n",
            "0.02643569053534884\n",
            "[Epoch 825 / 1000]\n",
            "0.026016931601974647\n",
            "[Epoch 826 / 1000]\n",
            "0.025969533947412856\n",
            "[Epoch 827 / 1000]\n",
            "0.026100000934908166\n",
            "[Epoch 828 / 1000]\n",
            "0.025390241285494993\n",
            "[Epoch 829 / 1000]\n",
            "0.026012627947726286\n",
            "[Epoch 830 / 1000]\n",
            "0.025876023204880767\n",
            "[Epoch 831 / 1000]\n",
            "0.025770077125809622\n",
            "[Epoch 832 / 1000]\n",
            "0.025541804635198787\n",
            "[Epoch 833 / 1000]\n",
            "0.025922902149439323\n",
            "[Epoch 834 / 1000]\n",
            "0.025695779144007246\n",
            "[Epoch 835 / 1000]\n",
            "0.02561569419625448\n",
            "[Epoch 836 / 1000]\n",
            "0.026101387040049304\n",
            "[Epoch 837 / 1000]\n",
            "0.025567576513974927\n",
            "[Epoch 838 / 1000]\n",
            "0.02645808132214006\n",
            "[Epoch 839 / 1000]\n",
            "0.025811111035640353\n",
            "[Epoch 840 / 1000]\n",
            "0.025964545587426984\n",
            "[Epoch 841 / 1000]\n",
            "0.025840792066883295\n",
            "[Epoch 842 / 1000]\n",
            "0.02599207188168657\n",
            "[Epoch 843 / 1000]\n",
            "0.026115980786853468\n",
            "[Epoch 844 / 1000]\n",
            "0.0259738190335338\n",
            "[Epoch 845 / 1000]\n",
            "0.02600796722283121\n",
            "[Epoch 846 / 1000]\n",
            "0.026110418949392623\n",
            "[Epoch 847 / 1000]\n",
            "0.02616874730109703\n",
            "[Epoch 848 / 1000]\n",
            "0.026239708640554454\n",
            "[Epoch 849 / 1000]\n",
            "0.026061471913708376\n",
            "[Epoch 850 / 1000]\n",
            "0.025862615212099628\n",
            "[Epoch 851 / 1000]\n",
            "0.026366708221030422\n",
            "[Epoch 852 / 1000]\n",
            "0.025967803963634652\n",
            "[Epoch 853 / 1000]\n",
            "0.025965285977756138\n",
            "[Epoch 854 / 1000]\n",
            "0.02596207015996333\n",
            "[Epoch 855 / 1000]\n",
            "0.02611809108866146\n",
            "[Epoch 856 / 1000]\n",
            "0.025741926171118392\n",
            "[Epoch 857 / 1000]\n",
            "0.025684812467137816\n",
            "[Epoch 858 / 1000]\n",
            "0.026364538517897017\n",
            "[Epoch 859 / 1000]\n",
            "0.026336895287036897\n",
            "[Epoch 860 / 1000]\n",
            "0.025847689063346477\n",
            "[Epoch 861 / 1000]\n",
            "0.026065229155356064\n",
            "[Epoch 862 / 1000]\n",
            "0.02648114256450208\n",
            "[Epoch 863 / 1000]\n",
            "0.02599140300869476\n",
            "[Epoch 864 / 1000]\n",
            "0.026092832402558996\n",
            "[Epoch 865 / 1000]\n",
            "0.02577740951991291\n",
            "[Epoch 866 / 1000]\n",
            "0.026047105921665207\n",
            "[Epoch 867 / 1000]\n",
            "0.02543849210056942\n",
            "[Epoch 868 / 1000]\n",
            "0.026323270421591587\n",
            "[Epoch 869 / 1000]\n",
            "0.025809961325256155\n",
            "[Epoch 870 / 1000]\n",
            "0.025687777878541963\n",
            "[Epoch 871 / 1000]\n",
            "0.02602870349562727\n",
            "[Epoch 872 / 1000]\n",
            "0.02600365909759421\n",
            "[Epoch 873 / 1000]\n",
            "0.025553286723035855\n",
            "[Epoch 874 / 1000]\n",
            "0.025860062946216202\n",
            "[Epoch 875 / 1000]\n",
            "0.02607084920804482\n",
            "[Epoch 876 / 1000]\n",
            "0.025540094155003315\n",
            "[Epoch 877 / 1000]\n",
            "0.02599275504378602\n",
            "[Epoch 878 / 1000]\n",
            "0.025794594742765183\n",
            "[Epoch 879 / 1000]\n",
            "0.025943138917675242\n",
            "[Epoch 880 / 1000]\n",
            "0.026095437087467872\n",
            "[Epoch 881 / 1000]\n",
            "0.025694077942112925\n",
            "[Epoch 882 / 1000]\n",
            "0.02589622036111541\n",
            "[Epoch 883 / 1000]\n",
            "0.026114640676125418\n",
            "[Epoch 884 / 1000]\n",
            "0.02556847858475521\n",
            "[Epoch 885 / 1000]\n",
            "0.02585569117104751\n",
            "[Epoch 886 / 1000]\n",
            "0.02591942088014912\n",
            "[Epoch 887 / 1000]\n",
            "0.025876535469549707\n",
            "[Epoch 888 / 1000]\n",
            "0.025555839024600572\n",
            "[Epoch 889 / 1000]\n",
            "0.02558557386434404\n",
            "[Epoch 890 / 1000]\n",
            "0.02544352292420808\n",
            "[Epoch 891 / 1000]\n",
            "0.026466542698268314\n",
            "[Epoch 892 / 1000]\n",
            "0.026020970706595106\n",
            "[Epoch 893 / 1000]\n",
            "0.026130707010626794\n",
            "[Epoch 894 / 1000]\n",
            "0.025767225847550434\n",
            "[Epoch 895 / 1000]\n",
            "0.02617653246142436\n",
            "[Epoch 896 / 1000]\n",
            "0.02594952730217483\n",
            "[Epoch 897 / 1000]\n",
            "0.025724095679703168\n",
            "[Epoch 898 / 1000]\n",
            "0.026475433257524855\n",
            "[Epoch 899 / 1000]\n",
            "0.026341911134077235\n",
            "[Epoch 900 / 1000]\n",
            "0.026466940979007633\n",
            "[Epoch 901 / 1000]\n",
            "0.025771323062363082\n",
            "[Epoch 902 / 1000]\n",
            "0.026295435122796335\n",
            "[Epoch 903 / 1000]\n",
            "0.025557546095107683\n",
            "[Epoch 904 / 1000]\n",
            "0.025951282249763608\n",
            "[Epoch 905 / 1000]\n",
            "0.025560126426396892\n",
            "[Epoch 906 / 1000]\n",
            "0.02600501500646351\n",
            "[Epoch 907 / 1000]\n",
            "0.026138603991246782\n",
            "[Epoch 908 / 1000]\n",
            "0.02590518619667273\n",
            "[Epoch 909 / 1000]\n",
            "0.02580584758578334\n",
            "[Epoch 910 / 1000]\n",
            "0.02647010803775629\n",
            "[Epoch 911 / 1000]\n",
            "0.025963519128388724\n",
            "[Epoch 912 / 1000]\n",
            "0.025251340032671578\n",
            "[Epoch 913 / 1000]\n",
            "0.025695117008523085\n",
            "[Epoch 914 / 1000]\n",
            "0.025728094503399915\n",
            "[Epoch 915 / 1000]\n",
            "0.02573508104175562\n",
            "[Epoch 916 / 1000]\n",
            "0.026224842834519223\n",
            "[Epoch 917 / 1000]\n",
            "0.026614061946049334\n",
            "[Epoch 918 / 1000]\n",
            "0.025521665223059244\n",
            "[Epoch 919 / 1000]\n",
            "0.02684229813993443\n",
            "[Epoch 920 / 1000]\n",
            "0.025494732928229496\n",
            "[Epoch 921 / 1000]\n",
            "0.02573038075811928\n",
            "[Epoch 922 / 1000]\n",
            "0.025625205864897\n",
            "[Epoch 923 / 1000]\n",
            "0.025542032997356728\n",
            "[Epoch 924 / 1000]\n",
            "0.025814930725609884\n",
            "[Epoch 925 / 1000]\n",
            "0.02604338937456487\n",
            "[Epoch 926 / 1000]\n",
            "0.02623128601355711\n",
            "[Epoch 927 / 1000]\n",
            "0.025394806421129033\n",
            "[Epoch 928 / 1000]\n",
            "0.026109390884521418\n",
            "[Epoch 929 / 1000]\n",
            "0.02625856706581544\n",
            "[Epoch 930 / 1000]\n",
            "0.02594553689006716\n",
            "[Epoch 931 / 1000]\n",
            "0.026145200599799863\n",
            "[Epoch 932 / 1000]\n",
            "0.025862124048377153\n",
            "[Epoch 933 / 1000]\n",
            "0.02572899821633473\n",
            "[Epoch 934 / 1000]\n",
            "0.026154319224646314\n",
            "[Epoch 935 / 1000]\n",
            "0.02608348999643931\n",
            "[Epoch 936 / 1000]\n",
            "0.025748328777262942\n",
            "[Epoch 937 / 1000]\n",
            "0.02604226536588976\n",
            "[Epoch 938 / 1000]\n",
            "0.02608476775814779\n",
            "[Epoch 939 / 1000]\n",
            "0.025898156693321652\n",
            "[Epoch 940 / 1000]\n",
            "0.02582510225020815\n",
            "[Epoch 941 / 1000]\n",
            "0.026353464421466924\n",
            "[Epoch 942 / 1000]\n",
            "0.025807501107570716\n",
            "[Epoch 943 / 1000]\n",
            "0.025710791663383133\n",
            "[Epoch 944 / 1000]\n",
            "0.02597409463248914\n",
            "[Epoch 945 / 1000]\n",
            "0.0255965492829564\n",
            "[Epoch 946 / 1000]\n",
            "0.025618261415103916\n",
            "[Epoch 947 / 1000]\n",
            "0.02625910735526122\n",
            "[Epoch 948 / 1000]\n",
            "0.026292710888083094\n",
            "[Epoch 949 / 1000]\n",
            "0.025986768186150585\n",
            "[Epoch 950 / 1000]\n",
            "0.02552517609996721\n",
            "[Epoch 951 / 1000]\n",
            "0.02556548372574616\n",
            "[Epoch 952 / 1000]\n",
            "0.025720109020825475\n",
            "[Epoch 953 / 1000]\n",
            "0.025656122139655055\n",
            "[Epoch 954 / 1000]\n",
            "0.02637375230283942\n",
            "[Epoch 955 / 1000]\n",
            "0.025970057777594775\n",
            "[Epoch 956 / 1000]\n",
            "0.025974770440836438\n",
            "[Epoch 957 / 1000]\n",
            "0.02581637514475733\n",
            "[Epoch 958 / 1000]\n",
            "0.025862536852248014\n",
            "[Epoch 959 / 1000]\n",
            "0.025328662195708604\n",
            "[Epoch 960 / 1000]\n",
            "0.025443507842108373\n",
            "[Epoch 961 / 1000]\n",
            "0.02575006942229811\n",
            "[Epoch 962 / 1000]\n",
            "0.025994458712171764\n",
            "[Epoch 963 / 1000]\n",
            "0.025831844535656272\n",
            "[Epoch 964 / 1000]\n",
            "0.02600335374212591\n",
            "[Epoch 965 / 1000]\n",
            "0.02586703822587151\n",
            "[Epoch 966 / 1000]\n",
            "0.025779521427699365\n",
            "[Epoch 967 / 1000]\n",
            "0.026187868801411242\n",
            "[Epoch 968 / 1000]\n",
            "0.02510747039655689\n",
            "[Epoch 969 / 1000]\n",
            "0.025547560809878633\n",
            "[Epoch 970 / 1000]\n",
            "0.02586542907531839\n",
            "[Epoch 971 / 1000]\n",
            "0.025660567185288526\n",
            "[Epoch 972 / 1000]\n",
            "0.025813197730341925\n",
            "[Epoch 973 / 1000]\n",
            "0.02625927008962026\n",
            "[Epoch 974 / 1000]\n",
            "0.026357630237471313\n",
            "[Epoch 975 / 1000]\n",
            "0.025301860480627512\n",
            "[Epoch 976 / 1000]\n",
            "0.02544171619869303\n",
            "[Epoch 977 / 1000]\n",
            "0.026108406434883363\n",
            "[Epoch 978 / 1000]\n",
            "0.025688572637620383\n",
            "[Epoch 979 / 1000]\n",
            "0.025560011954279618\n",
            "[Epoch 980 / 1000]\n",
            "0.026134252348565496\n",
            "[Epoch 981 / 1000]\n",
            "0.02623548462463077\n",
            "[Epoch 982 / 1000]\n",
            "0.026043987893266604\n",
            "[Epoch 983 / 1000]\n",
            "0.025987034820427652\n",
            "[Epoch 984 / 1000]\n",
            "0.026175570859806613\n",
            "[Epoch 985 / 1000]\n",
            "0.025900032174307853\n",
            "[Epoch 986 / 1000]\n",
            "0.02620908799848985\n",
            "[Epoch 987 / 1000]\n",
            "0.026436644513043575\n",
            "[Epoch 988 / 1000]\n",
            "0.02621951804542914\n",
            "[Epoch 989 / 1000]\n",
            "0.02600011314812582\n",
            "[Epoch 990 / 1000]\n",
            "0.02568519242538605\n",
            "[Epoch 991 / 1000]\n",
            "0.02582753836386837\n",
            "[Epoch 992 / 1000]\n",
            "0.026087161318282596\n",
            "[Epoch 993 / 1000]\n",
            "0.02646458152314881\n",
            "[Epoch 994 / 1000]\n",
            "0.02592933300510049\n",
            "[Epoch 995 / 1000]\n",
            "0.0256199085006956\n",
            "[Epoch 996 / 1000]\n",
            "0.02614471264090389\n",
            "[Epoch 997 / 1000]\n",
            "0.025470744009420742\n",
            "[Epoch 998 / 1000]\n",
            "0.025574803259805778\n",
            "[Epoch 999 / 1000]\n",
            "0.025952970486017874\n",
            "Bleu score 29.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9v_QVgVWQ5R",
        "outputId": "be031c35-35d4-4b38-da6d-780e67e82f62"
      },
      "source": [
        "score = bleu(test_data[1:100], model, chinese, english, device)\n",
        "print(f\"Bleu score {score * 100:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bleu score 30.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EspiHpq2XIH8"
      },
      "source": [
        "sentence = \"你可以帮我一个忙吗\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J405fa68XDHM",
        "outputId": "955ebef9-96e8-4b0d-ef6b-10929142fada"
      },
      "source": [
        "    translated_sentence = translate_sentence(\n",
        "        model, sentence, chinese, english, device, max_length=50\n",
        "    )\n",
        "\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translated example sentence: \n",
            " ['you', 'can', 'help', 'me', 'a', 'busy', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxb_aNk7659y"
      },
      "source": [
        "    if save_model:\n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "        }\n",
        "        save_checkpoint(checkpoint)\n",
        "\n",
        "    model.eval()\n",
        "    translated_sentence = translate_sentence(\n",
        "        model, sentence, chinese, english, device, max_length=50\n",
        "    )\n",
        "\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}